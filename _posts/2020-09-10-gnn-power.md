---
title: Paper Recap -- A Survey on the Expressive Power of Graph Neural Networks 
date: 2020-09-10
categories:
- Graph Neural Networks 
- Graph Theory 
- Representation Learning 
---

**Published:** 12.05.2020

**Paper Author:** Ryoma Sato

**Paper:** [arXiv](https://arxiv.org/pdf/2003.04078.pdf)

# TLDR

* This is a review paper on theoretical guarantees of Graph Neural Networks (GNNs).
* Message passing GNNs are linked to the Weisfeiler Lehman (WL) graph isomorphism test.
* Several variants of vanilla GNNs are proposed with guarantees to increasingly powerful WL tests by considering larger subgstrutures and improving the injective properties of aggregation functions. 

# Problem

The goal of this work is to explore the theoretical limitations of graph neural networks (GNNs) and propose alternatives with more powerful theoretical guarantees.
Typically, the power of a GNN, which we can think of a function which maps a graph to a point in a certain vector space, is measured by its ability to distinguish non-isomorphic graphs.
The most powerful GNN is therefore the one that can produce a different output for all non-isomorphic graphs.
Most currently used GNNs, while they show good performance empirically, are not able to fully distinguish all classes of graphs which is a direct limitation on their capacity as learners on graphs. 


# Approach


# Results
